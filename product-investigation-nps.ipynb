{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Product Investigation:\n",
    "\n",
    "This example demonstrates how a product owner can evaluate customer satisfaction from online reviews at scale. This process provides managers with higher fidelity information about the online market's sentiment for a product with specific justifications, allowing for more timely, accurate business decisions. \n",
    "\n",
    "Process: a product was selected at random from those containing over 75 reviews. Reviews were then segmented for investigation by sentiment analysis score. Exploring the review score's top and bottom tiers, clear insights emerge for actionable improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim\n",
    "\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt\n",
    "import scipy.sparse as ss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import:\n",
    "\n",
    "This data for this project were stored in MongoDB on AWS EC2 and accessed using the PyMongo module. The following code establishes connection, extracts a user-defined number of records from the electronics reviews collection, scans for null values, and creates a new dataframe for the columns of interest.\n",
    "\n",
    "Star reviews are scaled to Net Promoter Scores with the following assumptions: 5 stars equates to a promoter, 4 stars is neutral, and 1-3 stars is a detractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Mongo on AWS and pull data:\n",
    "\n",
    "client = MongoClient('mongodb://username@1234567890:27017/amazon_reviews', password = '1234567890')\n",
    "\n",
    "db = client.amazon_reviews\n",
    "    \n",
    "try:\n",
    "    if db.list_collection_names() == ['electronics']:\n",
    "        print(\"Connected to MongoDB on AWS EC2.\\n\")\n",
    "    \n",
    "    total_records = int(input('Specify number of records:'))\n",
    "        \n",
    "    # pull reviews from electronics category to load dataframe:\n",
    "\n",
    "    elec = list(db.electronics.find().limit(total_records))\n",
    "    df = pd.DataFrame(list(elec))\n",
    "    print('>>', df.shape[0],'reviews retrieved from database\\n')\n",
    "\n",
    "except:\n",
    "    print(\"Connection Error.\")\n",
    "    \n",
    "check_null = df.isnull().values.any()\n",
    "print('Null values in df =', check_null, '\\n')\n",
    "\n",
    "df = df[['product_id', 'product_title', 'review_body', 'review_headline', 'star_rating', 'total_votes', 'verified_purchase', 'vine']]\n",
    "\n",
    "# Assign 5 star reviews to 1, 4 star reviews to 0, and 1-3 star reviews to -1:\n",
    "\n",
    "df['nps'] = np.where(df['star_rating'] == 5, 1, np.where(df['star_rating'] == 4, 0, -1))\n",
    "\n",
    "print('Unique products:', len(list(df.product_id.unique())),'\\n')\n",
    "\n",
    "df.to_pickle('df_records.pkl') \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of products with > 75 reviews:\n",
    "\n",
    "df_counts = df.groupby(['product_id']).count().sort_values('product_title', ascending = False)\n",
    "sort_mask = (df_counts['product_title'] > 75)\n",
    "\n",
    "df_counts[sort_mask].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this example, the 3rd product was \"randomly\" selected:\n",
    "\n",
    "product_mask = (df['product_id'] == 'B00F5NE2KG')\n",
    "df_product = df[product_mask]\n",
    "df_product.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis:\n",
    "\n",
    "As is often the case with online reviews, this product's ratings are skewed left with median equal to the max of 5 stars. As a derivative of star ratings, Net Promoter Score follows a similar pattern. With both medians also equal to the highest rating, we can hypothesize that there is signal loss in simplified ratings systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product.star_rating.hist()\n",
    "\n",
    "print(\"Mean Star Rating:\", df_product.star_rating.mean())\n",
    "print(\"Median Star Rating:\", df_product.star_rating.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product.nps.hist()\n",
    "\n",
    "print(\"Mean NPS Rating:\", df_product.nps.mean())\n",
    "print(\"Median NPS Rating:\", df_product.nps.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Pre-processing:\n",
    "\n",
    "The text data are processed with spaCy and loaded to a new column. The functions `alphanumeric` and `punc_lower` remove unwanted punctuation and convert text to lowercase, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('/home/ubuntu/anaconda3/lib/python3.7/site-packages/en_core_web_sm/en_core_web_sm-2.1.0/')\n",
    "\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "stop_words.update(['-PRON-', 'pron', 'br', '<br/>', '<br />', '<br>', '<br', 'br>', '</br', 'br/>', '<br /><br />', 'use', 'like', '/br', 'br/'])\n",
    "\n",
    "# Prepare data to process with Spacy:\n",
    "\n",
    "list_of_spacy = []\n",
    "\n",
    "for document in nlp.pipe(df_product.review_body, n_threads = -1):\n",
    "    try:\n",
    "        list_of_spacy.append(document)\n",
    "        \n",
    "    except:\n",
    "        list_of_spacy.append('bad_doc')\n",
    "        print('bad doc:', document)\n",
    "    \n",
    "df_product['review_body_spacy'] = list_of_spacy\n",
    "\n",
    "print(f'{len(list_of_spacy)} documents processed with Spacy\\n')\n",
    "\n",
    "# pre-process text with NLTK for Corex:\n",
    "\n",
    "alphanumeric = lambda x: re.sub(r'<[^>]*>\\w*\\d\\w*', '', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x.lower())\n",
    "\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "nltk_list = []\n",
    "\n",
    "df_product['cleaned_review_body_text'] = df_product['review_body_spacy'].apply\\\n",
    "(lambda x: ' '.join([ele.lemma_ for ele in x if ele.lemma_ not in stop_words and ele not in stop_words]))\n",
    "\n",
    "for row in df_product['cleaned_review_body_text']:\n",
    "    nltk_list.append(' '.join(w for w in nltk.wordpunct_tokenize(row) if w.lower() in words or not w.isalpha()))\n",
    "    \n",
    "df_product['nltk_terms'] = nltk_list\n",
    "\n",
    "df_product['nltk_terms'] = df_product['nltk_terms'].map(alphanumeric).map(punc_lower)\n",
    "\n",
    "df_product = df_product.drop(columns = ['cleaned_review_body_text'])\n",
    "\n",
    "print('NLTK processing complete.')\n",
    "\n",
    "df_product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Hypothesis Test:\n",
    "\n",
    "Before delving into a protracted NLP analysis to find fidelity in language where the ratings have disappointed, it is important to determine whether a general correlation between product reviews and ratings exists. \n",
    "\n",
    "This will be accomplished by applying classification to a Bag-of-Words model, specifically using CountVectorizer and Logistic Regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with review body text and nps:\n",
    "\n",
    "df2 = df[['review_body', 'nps']]\n",
    "df2 = df2.drop(df2[df2.nps == 0].index) # remove neutral rating (4 stars) for binary classification\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features & labels:\n",
    "\n",
    "X = df2.review_body.values.astype('unicode')\n",
    "y = df2.nps\n",
    "\n",
    "# split into training / test sets:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# The first document-term matrix has default Count Vectorizer values - counts of unigrams:\n",
    "\n",
    "cv1 = CountVectorizer(max_features=20000, stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", binary=True)\n",
    "\n",
    "X_train_cv1 = cv1.fit_transform(X_train)\n",
    "X_test_cv1  = cv1.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train_cv1.toarray(), columns=cv1.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a second document-term matrix has both unigrams and bigrams, and indicators instead of counts:\n",
    "\n",
    "cv2 = CountVectorizer(max_features=20000, stop_words='english', ngram_range=(1,2), token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", binary=True)\n",
    "                    \n",
    "X_train_cv2 = cv2.fit_transform(X_train)\n",
    "X_test_cv2  = cv2.transform(X_test)\n",
    "\n",
    "pd.DataFrame(X_train_cv2.toarray(), columns=cv2.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression model to use\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Train the first model\n",
    "lr.fit(X_train_cv1, y_train)\n",
    "y_pred_cv1 = lr.predict(X_test_cv1)\n",
    "\n",
    "\n",
    "# Train the second model\n",
    "lr.fit(X_train_cv2, y_train)\n",
    "y_pred_cv2 = lr.predict(X_test_cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the error metrics, since we'll be doing this several times\n",
    "\n",
    "def conf_matrix(actual, predicted):\n",
    "    cm = confusion_matrix(actual, predicted)\n",
    "    sns.heatmap(cm, xticklabels=['predicted_negative', 'predicted_positive'], \n",
    "                yticklabels=['actual_negative', 'actual_positive'], annot=True,\n",
    "                fmt='d', annot_kws={'fontsize':20}, cmap=\"YlGnBu\");\n",
    "\n",
    "    true_neg, false_pos = cm[0]\n",
    "    false_neg, true_pos = cm[1]\n",
    "\n",
    "    accuracy = round((true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg),3)\n",
    "    precision = round((true_pos) / (true_pos + false_pos),3)\n",
    "    recall = round((true_pos) / (true_pos + false_neg),3)\n",
    "    f1 = round(2 * (precision * recall) / (precision + recall),3)\n",
    "\n",
    "    cm_results = [accuracy, precision, recall, f1]\n",
    "    return cm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "The supervised learning trial confirms a strong connection between reviews and ratings with all metrics above 90% and F1 scores nearing 94%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat map for the first logistic regression model:\n",
    "\n",
    "cm1 = conf_matrix(y_test, y_pred_cv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heat map for the second logistic regression model\n",
    "\n",
    "cm2 = conf_matrix(y_test, y_pred_cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results:\n",
    "\n",
    "results = pd.DataFrame(list(zip(cm1, cm2)))\n",
    "results = results.set_index([['Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "results.columns = ['LogReg1', 'LogReg2']\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VADER Sentiment Analysis:\n",
    "\n",
    "The star ratings & NPS data's extreme left skew exhibits a bias for 5 Star Ratings. How then does a product discern between those 5 Star Ratings from lethargic or generous customers and those reviews indicative of raving fans?\n",
    "\n",
    "The most enthusiastic fans were isolated from the \"5-Star by Default\" population using Valence Aware Dictionary and sEntiment Reasoner (VADER) Sentiment Analysis. A filter coupling VADER scores over 0.95 with an NPS Score of 1 (as a checksum for sarcasm) was then used to populate a dataframe comprised of our most satisfied customers. This process was then repeated with inverse parameters to isolate the least satisfied customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use VADER with original reviews column as input to include emotionally rich punctuation and capitalization: \n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# compound is the normalized, weighted composite score:\n",
    "\n",
    "df_product['vader'] = [analyzer.polarity_scores(row)['compound'] for row in df_product.review_body] \n",
    "\n",
    "print(f'created VADER scores for {df_product.vader.shape[0]} rows\\n')\n",
    "\n",
    "df_product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product.vader.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore top reviews using a vader filter > 0.95:\n",
    "\n",
    "df_p1_top_mask = (df_product['vader'] > 0.95)\n",
    "\n",
    "df_product[df_p1_top_mask].nps.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product[df_p1_top_mask].star_rating.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a combination of vader and nps to create dataframe for top and bottom reviews:\n",
    "\n",
    "df_p1_top_mask = (df_product['vader'] > 0.95) & (df_product['nps'] == 1)\n",
    "df_top = df_product[df_p1_top_mask]\n",
    "\n",
    "df_p1_bot_mask = (df_product['vader'] < -0.25) & (df_product['nps'] == -1)\n",
    "df_bot = df_product[df_p1_bot_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_vectorizer = CountVectorizer(max_features=20000, ngram_range=(1,2), binary=True, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", stop_words='english')\n",
    "\n",
    "cor_doc_word_top = cor_vectorizer.fit_transform(df_top['nltk_terms'])\n",
    "cor_words = list(np.asarray(cor_vectorizer.get_feature_names())) \n",
    "topic_model_top = ct.Corex(n_hidden=6, words = cor_words, seed=1)\n",
    "topic_model_top.fit(cor_doc_word_top, words = cor_words, docs = df_top.nltk_terms)\n",
    "\n",
    "# repeat process for bottom reveiws:\n",
    "\n",
    "cor_doc_word_bot = cor_vectorizer.fit_transform(df_bot['nltk_terms'])\n",
    "cor_words = list(np.asarray(cor_vectorizer.get_feature_names()))\n",
    "topic_model_bot = ct.Corex(n_hidden=6, words = cor_words, seed=1) # must be repeated\n",
    "topic_model_bot.fit(cor_doc_word_bot, words = cor_words, docs = df_bot.nltk_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Evaluation: Top Reviews\n",
    "\n",
    "CorEx topic models operate on the principle of mutual information between topic words and the topic - not the highest probability of a topic word occurring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all topics from the top topic model:\n",
    "\n",
    "topics = topic_model_top.get_topics()\n",
    "for n, topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve original reviews & top topics for parameters specified:\n",
    "\n",
    "def TopicReviews(topic_number, topic_model_version, topic_dataframe):\n",
    "    \n",
    "    print('Topics: \\n\\n', topic_model_version.get_topics()[topic_number])\n",
    "\n",
    "    top_docs = topic_model_version.get_top_docs(topic=topic_number, n_docs=3)\n",
    "\n",
    "    for i in top_docs:\n",
    "        temp = (topic_dataframe.loc[topic_dataframe['nltk_terms'] == i[0]]).index[0]  # get row index of review for original review\n",
    "        print('\\nIndex:', temp)\n",
    "        print('Rating:', df.star_rating.iloc[temp])\n",
    "        print('Review:', df.review_body.iloc[temp])  # print original review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 0: \n",
    "\n",
    "In Topic 0, we can see that the word \"quality\" has the highest mutual information and sets the general topic theme. From Topic 0's other top words, we can find other specific justifications for why customers wrote enthusiastically positive reviews: \n",
    "\n",
    "* convenient\n",
    "* simple\n",
    "* works in wet environments (sauna)\n",
    "\n",
    "CorEx also allows us to `get_top_docs` to read the reviews most highly correlated with a topic. Since the text processing makes the CorEx input reviews difficult to read, I've indexed the original reviews for legibility.\n",
    "\n",
    "The top 3 reviews give us a better idea of what is meant by the \"quality\" topic: sound. Customers also like the size and portability.\n",
    "From a marketing perspective, these reviews that best define our quality topic tell us that customers are purchasing the product for their kids and as gifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicReviews(0, topic_model_top, df_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 1:\n",
    "\n",
    "Topic 1's general theme is **exceeds expectations**. Not only is \"*expect*\" the top topic words, but the top documents show a relationship between its \"smaller than expected\" size producing \"more than we expected for sound quality.\" This is all given, \"I did not expect much given its moderate price.\" For a product named \"MagicBox,\" exceeding expectations is brand-consistent.\n",
    "\n",
    "Topic 1 also reinforces insights from Topic 0: customers are buying the speaker for their children, giving it as gifts, and using it in wet environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicReviews(1, topic_model_top, df_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model_top.tcs.shape[0]), topic_model_top.tcs, color='#ffa500', width=0.5)\n",
    "plt.title('Correlation by Topic: Top Reviews')\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Evaluation: Bottom Reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all topics from the bottom topic model:\n",
    "\n",
    "topics = topic_model_bot.get_topics()\n",
    "for n, topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 0:\n",
    "\n",
    "This topic words show clustering around sound issues when using Bluetooth:\n",
    "\n",
    "* high frequency noise\n",
    "* audio quality\n",
    "* pairing & range\n",
    "\n",
    "These findings are surprising considering that most reviews are positive and mention superb sound quality. Further, 2 of the top documents mention the sound quality improving when operating from an auxillary cable. \n",
    "\n",
    "This topic reveals a potential manufacturing or component problem involving Bluetooth that is detracting from product performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicReviews(0, topic_model_bot, df_bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic 1:\n",
    "\n",
    "Topic 1 shares the defective product theme with Topic 0 and it follows that a top document is also repeated. However, this topic offers more information as to how the product is failing: \n",
    "\n",
    "* broken charge port\n",
    "* defective volume controls\n",
    "\n",
    "Since this product was sold online and reviews can be mapped to customers, the management has the ability to investigate supply chain and / or manufacturing patterns causing this defective lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopicReviews(1, topic_model_bot, df_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(topic_model_bot.tcs.shape[0]), topic_model_bot.tcs, color='#4e79a7', width=0.5)\n",
    "plt.title('Correlation by Topic: Bottom Reviews')\n",
    "plt.xlabel('Topic', fontsize=16)\n",
    "plt.ylabel('Total Correlation (nats)', fontsize=16);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
